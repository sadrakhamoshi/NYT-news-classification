# -*- coding: utf-8 -*-
"""bert classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fl72c6URaBbVAAxYVZnpclDfGzE63Pkg

#Requirements

## Install Packages
"""

!pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d rmisra/news-category-dataset

! unzip news-category-dataset.zip

!pip install -q -U "tensorflow-text==2.8.*"

from google.colab import drive
drive.mount('/content/drive')

!pip install -q tf-models-official==2.7.0

"""## Importing Packages"""

import pandas as pd
import os
import shutil
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization 

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')

"""#NEWS - last"""

df = pd.read_json('/content/News_Category_Dataset_v2.json', lines=True)
df.head()

cates = df.groupby('category')
print("total categories:", cates.ngroups)
print(cates.size())

df['text'] = df.headline + " " + df.short_description

news = df['text'].values
cat = df['category'].values
news.shape, cat.shape

LABEL_USED_COUNT = 12
label_used = df.category.value_counts().index[:LABEL_USED_COUNT]
print('label used for prediction the {}'.format(label_used))

df_top_category = df[df['category'].isin(label_used)]
df_top_category

news = df_top_category['text'].values
cats = df_top_category['category'].values

category_str = np.unique(cats)
cat_to_index = {val:idx for idx, val in enumerate(category_str)}
CLASS_NUMBERS = len(category_str)
print(cat_to_index), len(category_str)

y = np.array([cat_to_index.get(key) for key in cats])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(news, y, test_size=0.25)

y_train = tf.keras.utils.to_categorical(y_train, num_classes=CLASS_NUMBERS)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=CLASS_NUMBERS)

"""#NYT DATA

## Load data
Load the data which preprocessed in previous phase. Data has 33 different classes of news which shuffled.
"""

nyt_data_path = '/content/drive/MyDrive/nlp/cleaned_news.csv'

df = pd.read_csv(nyt_data_path)

nyt_news = df['news'].values
nyt_cat = df['category'].values
nyt_news.shape, nyt_cat.shape

nyt_category_str = np.unique(nyt_cat)
cat_to_index = {val:idx for idx, val in enumerate(nyt_category_str)}
NYT_CLASS_NUMBERS = len(nyt_category_str)
print(cat_to_index), len(nyt_category_str)

nyt_y = np.array([cat_to_index.get(key) for key in nyt_cat])
nyt_y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(nyt_news, nyt_y, test_size=0.25)

"""Labels must be in one-hot encoding."""

y_train = tf.keras.utils.to_categorical(y_train, num_classes=33)

y_test = tf.keras.utils.to_categorical(y_test, num_classes=33)

"""# NEWS DATA

This Data has 4 different classes of news.
"""

DATASET_PATH = '/content/drive/MyDrive/Ag_news_dataset'

TRAIN_DATA_PATH = DATASET_PATH + '/cleaned_train_data.csv'
TEST_DATA_PATH = DATASET_PATH + '/cleaned_test_data.csv'

df_train = pd.read_csv(TRAIN_DATA_PATH)
df_test = pd.read_csv(TEST_DATA_PATH)
df_test.head()

X_train = df_train['description']
X_test = df_test['description']

num_classes = 4

y_train = tf.keras.utils.to_categorical(df_train['category'].values - 1, num_classes=num_classes)
y_test = tf.keras.utils.to_categorical(df_test['category'].values - 1, num_classes=num_classes)

print(len(X_train), len(X_test))
print(len(y_train), len(y_test))

"""#model

For text classification we used BERT. There is a preprocessor and encoder which defined below. A dropout layer is Added with a classifer.
"""

tf.keras.utils.plot_model(classifier_model)

import tensorflow_hub as hub
import tensorflow_text as text

preprocessor = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1")

def get_embeddings(sentences):
  '''return BERT-like embeddings of input text
  Args:
    - sentences: list of strings
  Output:
    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)
  '''
  preprocessed_text = preprocessor(sentences)
  return encoder(preprocessed_text)['pooled_output']

from keras import backend as K

def balanced_recall(y_true, y_pred):
    """This function calculates the balanced recall metric
    recall = TP / (TP + FN)
    """
    recall_by_class = 0
    # iterate over each predicted class to get class-specific metric
    for i in range(y_pred.shape[1]):
        y_pred_class = y_pred[:, i]
        y_true_class = y_true[:, i]
        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        recall_by_class = recall_by_class + recall
    return recall_by_class / y_pred.shape[1]

def balanced_precision(y_true, y_pred):
    """This function calculates the balanced precision metric
    precision = TP / (TP + FP)
    """
    precision_by_class = 0
    # iterate over each predicted class to get class-specific metric
    for i in range(y_pred.shape[1]):
        y_pred_class = y_pred[:, i]
        y_true_class = y_true[:, i]
        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        precision_by_class = precision_by_class + precision
    # return average balanced metric for each class
    return precision_by_class / y_pred.shape[1]

def balanced_f1_score(y_true, y_pred):
    """This function calculates the F1 score metric"""
    precision = balanced_precision(y_true, y_pred)
    recall = balanced_recall(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

num_classes = 12
i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
x = preprocessor(i)
x = encoder(x)
x = tf.keras.layers.Dropout(0.2, name="dropout")(x['pooled_output'])
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(num_classes, activation='softmax', name="output")(x)

model = tf.keras.Model(i, x)

n_epochs = 10

METRICS = [
      tf.keras.metrics.CategoricalAccuracy(),
      balanced_recall,
      balanced_precision,
      balanced_f1_score
]

earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = "val_loss", 
                                                      patience = 3,
                                                      restore_best_weights = True)

model.compile(optimizer = "adam",
              loss = "categorical_crossentropy",
              metrics = METRICS)

model_fit = model.fit(X_train, 
                      y_train, 
                      epochs = n_epochs,
                      validation_data = (X_test, y_test),
                      callbacks = [earlystop_callback],
                      batch_size = 32)

model.save("/content/drive/MyDrive/Ag_news_dataset/bert_text_classifier_NYT")

n_epochs = 20

METRICS = [
      tf.keras.metrics.CategoricalAccuracy(),
      balanced_recall,
      balanced_precision,
      balanced_f1_score
]

earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = "val_loss", 
                                                      patience = 3,
                                                      restore_best_weights = True)

model.compile(optimizer = "adam",
              loss = "categorical_crossentropy",
              metrics = METRICS)

model_fit = model.fit(X_train, 
                      y_train, 
                      epochs = n_epochs,
                      validation_data = (X_test, y_test),
                      callbacks = [earlystop_callback])

n_epochs = 20

METRICS = [
      tf.keras.metrics.CategoricalAccuracy(),
      balanced_recall,
      balanced_precision,
      balanced_f1_score
]

earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = "val_loss", 
                                                      patience = 3,
                                                      restore_best_weights = True)

model.compile(optimizer = "adam",
              loss = "categorical_crossentropy",
              metrics = METRICS)

model_fit = model.fit(X_train, 
                      y_train, 
                      epochs = n_epochs,
                      validation_data = (X_test, y_test),
                      callbacks = [earlystop_callback])

n_epochs = 5

METRICS = [
      tf.keras.metrics.CategoricalAccuracy(),
      balanced_recall,
      balanced_precision,
      balanced_f1_score
]

earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = "val_loss", 
                                                      patience = 3,
                                                      restore_best_weights = True)

model.compile(optimizer = "adam",
              loss = "categorical_crossentropy",
              metrics = METRICS)

model_fit = model.fit(X_train, 
                      y_train, 
                      epochs = n_epochs,
                      validation_data = (X_test, y_test),
                      callbacks = [earlystop_callback])

model.save("/content/drive/MyDrive/nlp/bert_classifier_breaking_news")